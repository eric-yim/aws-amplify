

<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Machine Learning | Putting Friends' Faces on Yours</title>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" type="text/css" href="/static/projects/bootstrap.min.css">

  <!-- Custom fonts for this template -->
  <link href="/static/projects/all_new.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/static/projects/simple-line-icons_new.css">
  <link href="/static/projects/fonts/googleapis_Lato.txt" rel="stylesheet">
  <link href="/static/projects/fonts/googleapis_Catamaran.txt" rel="stylesheet">
  <link href="/static/projects/fonts/googleapis_Multi.txt" rel="stylesheet">

  <!-- Plugin CSS -->
  <link rel="stylesheet" href="/static/projects/device-mockups.min.css">

  <!-- Custom styles for this template -->
  <link href="/static/projects/new-age.min.css" rel="stylesheet">
  <style>

    header.masthead{position:relative;width:100%;padding-top:150px;padding-bottom:100px;color:#fff;background:url(/static/projects/images/bg-pattern.png),#7b4397;background:url(/static/projects/images/bg-pattern.png),linear-gradient(to left,#7b4397,#dc2430)}

  .center_img{
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
    margin-bottom:50px;
  }  
    </style>
</head>

<body id="page-top">

  <!-- Navigation -->
  
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand js-scroll-trigger" href="/">Yim Machine Learning</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/#projects">Projects</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/#contact">Contact</a>
        </li>
      </ul>
    </div>
  </div>
</nav>



<style>
header.masthead{
  min-height:10vh;max-height:50vh;padding-top:15vh;padding-bottom:0;
  }


.features a{
  color:#843d89;
}
</style>

<header class="masthead" >
    <div class="container h-100">
      <div class="row h-100">
        <div class="col-lg-12 my-auto">
          <div class="header-content mx-auto">
          
            <h1 class="mb-5 text-center">Putting Friends' Faces on Yours</h1>
          
          </div>
        </div>

      </div>
    </div>
  </header>

  <section class="features" id="about">
    <div class="container">
      <!-- colors purple#7c4191#843d89 red #dd2039  -->
      <div class="row">
        <div class="offset-lg-1 col-lg-10 ">
          <div class="device ">

          <!-- START BLOCK HERE-->
          
            <style>
    .codebox {
        /* Below are styles for the codebox (not the code itself) */
        border:1px solid black;
        background-color:#EEEEFF;
        width:300px;
        overflow:auto;    
        padding:10px;
    }
    .codebox code {
        /* Styles in here affect the text of the codebox */
        font-size:0.9em;
        /* You could also put all sorts of styling here, like different font, color, underline, etc. for the code. */
    }
</style>
<img src="/static/projects/images/style-face-video.gif" class="img-fluid center_img" alt="style face" >
<p>When a friend showed me a Snapchat Lens, which could give me an "anime" look, I was impressed. But I also wanted something else. If I were going to transform my face, I wanted to <i>really</i> transform my face. And I wanted control. So I decided to set out to build my own face morpher.</p>
<p><b>In this article, I discuss journey of building a face-changing machine learning model and putting it in the browser.</b></p>
<p>Here are the lessons I've learned along the way.</p>
<p><a class="js-scroll-trigger" href="#results">Skip to results</a></p>


<h2>Search Term: Awesome</h2>
<p>My first challenge was finding or training a machine learning model.</p>
<p>There's a trade off between using well-tested technologies and new research. On one hand, with the current super-sonic pace of AI development, even technologies a couple years old could be very behind. 
    On the other hand, reading a paper and trying to independently implement it feels something like assembling a jigsaw puzzle with every third piece missing.</p>
<p><b>The best solution is to find papers with code.</b> In this way, you can use cutting-edge technology without much of the headache of coding mathematical formulas, and you get to see the engineering tricks that are omitted from the paper. 
</p> <p>For the photo-to-anime task, there are good compilations of such papers you can find through an internet search. As it turns out, there are <i>awesome</i> compilations for a number of fields. I'm not sure where the nomenclature originated from, but it seems that on Github, <b>the term <i>awesome</i> refers to community-organized lists.</b> When you're doing your next browse of technologies, add this term into your search.</p>

<p>In looking through image-transforming technologies, I kept running into a research group called <a href='https://clova.ai/ko' target="_blank">CLOVA</a> and decided to use their method <a href="https://github.com/clovaai/stargan-v2" target="_blank">StarGAN v2</a>. 
    CLOVA has done a number of experiments, turning horses to zebras, turning cats into tigers, and of course, turning people into anime. Their StarGAN v2 demo below shows their technique to combining a style and a source image. In the Snapchat realm, the <i>style</i> would be what I get to look like, and the <i>source</i> would be my camera feed.</p>
<img src="/static/projects/images/style-celebahq_interpolation.gif" class="img-fluid center_img" alt="clova" style="margin-bottom:30px">
<p>In particular, their StarGAN paper discussed the task of merging human faces and of merging animal faces, each task being a separately trained model. Out of curiosity, I wondered what would happen if you used a model to do what it wasn't meant to do (see below). This is the <i>human model</i> used in animal situations. You can see it's essentially trying to place human features on the contour of the cat. Impressive (and also, yikes)!</p>
<img src="/static/projects/images/style-untrained.jpg" style='width:30%' class="img-fluid center_img" alt="untrained domain" style="margin-bottom:30px">
<h2>Stick with a Family of Frameworks</h2>
<p>In 2021, much of the machine learning world is working with one of two frameworks: Tensorflow or PyTorch. 
    Research is often done in the Python form of these two frameworks. However, models are often deployed in other languages such as c++, javascript, or languages built for mobile devices. We then have to port our Python model into a new format. Most likely your model will need to be converted into a new format, and your early decisions will determine the amount of headache you'll have later on.
    In other words, <b>how can we streamline the process of going from training to deployment?</b>
</p>
<div class='row' style="margin-bottom:30px">
    <div class='col-3 offset-3'>
        <img src="/static/projects/images/Tensorflow_logo.svg.png" class="img-fluid center_img" alt="tf" >

    </div>
    <div class='col-3'>
        <img src="/static/projects/images/pytorch-logo.png" class="img-fluid center_img" alt="py">

    </div>
</div>
<p>
    Some people may say not to worry because you can use formats like ONNX, a neural network standardization format. However, in practice, I found many of the built-in converters (i.e. Tensorflow-to-ONNX) run into errors if a neural network uses novel layer types. It's likely ONNX lags behind the modern developments.
    If you are forced to use ONNX because it's the format your deployment environment accepts (i.e. Snapchat's developer system), you will need to stick with models that don't have special layers or you will need to substitute the special layers with pieces of supported layers. And with all the delicate tuning involved in training machine learning models, I'm not confident in my capability for such manipulations.
</p>
<p>
    Fortunately, both Tensorflow and Pytorch have related families and extensive tutorials for deploying models. Tensorflow has Tensorflow.js for browsers and Tensorflow Lite for mobile devices. PyTorch has PyTorch Mobile. I'm not fully sure what the PyTorch method is for getting models into javascript, and if you know this, please teach me!
    In my experience, the same-family converters are much less buggy. <b>Start by asking how you want to deploy, and work backwards choosing the same family of frameworks.</b>
</p>
<p>
    I wanted to deploy in the browser. Why? I wanted to share my adventures with my friends (and also possible employers, *hint hint*). I knew I could much more easily get people to a link versus get them to download an app. 
    For my purposes, Tensorflow.js seemed the way to go. Therefore, I would be training and developing my model with Tensorflow.
</p>

<h2>Cloud GPUs Seem to be Scarce</h2>
<p>
    One problem I didn't expect to face in the project was a search for cloud compute.
    I have a GPU on my personal desktop, but turned out StarGAN has so many parameters that my GPU would run out of memory on batch size of 2, where the author's code defaults to a batch size of 8.
    For any non-machine-learning readers, a batch size is the number of data samples grouped into a training step. For example, if you had 40 data samples, and a batch size of 8, you would need 5 training steps to work through your data.
    You can see that one drawback for small batch size is that your training will take longer. More importantly, however, neural networks will learn drastically different parameters according to batch size. Imagine you are training a categorizer for cats and dogs, if you had a batch size of 8, in each training step, your model is asked to make generalizations among the 4 cats and 4 dogs. If you had a batch size of two, your model is asked to tell the difference between a single dog and a single cat, which allows for much less generalization. 
</p>
<p>
    Eventually I'd be changing the training to fit my own needs, but <b>for a first trial, I wanted to keep the training procedure as close to the author's original procedure as possible.</b>
</p>
<p>
    If you are a <del>lonely, sad, helpless,</del> one-man superhero like me, you will find <b>cloud GPUs are hard to come by</b>.
    This is because the default settings for accounts like mine have a GPU limit set to <i>ZERO</i>. 
    You need to contact the sales teams and convince them to upgrade your account. Even telling them about my GPU memory issue, I was only able to get them to commit to <i>not enough</i> resources.
    In short, they were saying, "you are small, so here's a small amount of GPU until you get bigger". 
</p>
<p>I eventually stumbled upon a company called Lambda Labs. The types of instances available were far fewer and extra functionality (i.e. like AWS Sagemaker) were non-existent, but this service was exactly what I was looking for. 
    It took a few days for their best-priced GPU to free up, but once it did, 
    I set up the virtual machine, trained the model, and downloaded my files with no hitch. Now, I finally had the model I could put into the browser!</p>

<h2 id="results">Now the Fun Part!</h2>
<p>After converting models to the proper format and tinkering with the javascript (which I'm pretty bad at still... btw, what is a promise?) available in demo examples, I was able to get my demo working. 
</p>
<img src="/static/projects/images/style-mon.png" style="width:50%" class="img-fluid center_img" alt="monf" >
   <p> The <i>cool</i> part about Tensorflow.js is that it downloads the model and then uses your local hardware to run the model. This means your video camera feed is staying local, and if you have GPU your model can run faster. 
   </p>
   <p>    The <i>uncool</i> part about Tensorflow.js is that it downloads the model and runs on your local hardware. For me, it takes a minute to load the initial model, which is about 60MB, and it seems the model is too big to run on a mobile device.
</p>

<div class='row' style="margin-bottom:30px">
    <div class='col-6'>
        <img src="/static/projects/images/style-eric-female.png" class="img-fluid center_img" alt="ericf" >

    </div>
    <div class='col-6'>
        <img src="/static/projects/images/style-eric-male.png" class="img-fluid center_img" alt="ericm">

    </div>
</div>

<p>Here, you give the page a <i>style</i> image, which determines some of the colors and textures that will be layers over the contours of your own face. My friends and I enjoyed toying with it using our faces. There's something intriguing about seeing what an AI will come up with. 
    I ask, "Why does it think my face is wrinkly?", the same way I'd question a child drawing my portrait.</p>
<p><b>There are a number of dataset-related limitations that are not initially obvious.</b></p>
<ul>
    <li>
        <b>Asian person images create non-asian person results.</b> I and many of my friends are Asian. However, the celebrity dataset used includes very few Asian people. If this were important to you, you could either add more Asian person images to the dataset. Alternatively, StarGAN's model allows for separating images into categories, and at deployment, you would specify from which category you are drawing.
    </li>
    <li>
        <b>Females are easier to make smile.</b> It's likely that the percentage of females smiling in the dataset is greater than that of males. When testing a variety of my friends' faces, I found I could more easily make the avatar smile if the avatar were female.
    </li>
    <li>
        <b>You can't blink.</b> This is something that is pretty obvious afterward, but something I hadn't thought of prior to implementing the model: if you want to give your avatar the ability to blink, you need to through a bunch of pictures into the dataset of people with their eyes closed. Of course, not many closed-eyed pictures of celebrities get circulated around.
    </li>
    <li>
        <b>Images should be nice and pretty.</b> The celebrity pictures are high quality and well-lit. Sometimes when using dark, low-res picture for the style image, the model will add wrinkles to the result. This is important only for the style image input, whereas the source (camera) input need not be as babied.
    </li>
</ul>

<p>My initial goal was to build a machine learning model which could rival Snapchat's Anime Lens. While I've not yet built an anime lens, I have taken a deep dive into the workings, capabilities, and limitations of StarGAN. I believe with little additional tweaking, the same procedures can be done using a person/anime dataset throught he domain categorization aspect of StarGAN. In the project, I've also successfully ported a model from training to deployment. Finally, and most importantly, I've built a demo that people can enjoy while bonding with their friends.</p>
                  
          <!-- ENDBLOCK -->
          </div>
        </div>
	      



      </div>
    </div>
  </section>






  <section class="contact bg-primary" id="contact">
    <div class="container">
      <h2>Create together!</h2>
      <ul class="list-inline list-social">
        <li class="list-inline-item social-facebook">
          <a href="https://www.linkedin.com/in/yimeric/">
            <i class="fab fa-linkedin"></i>
          </a>
        </li>
        <li class="list-inline-item social-twitter">
          <a href="https://github.com/eric-yim/">
            <i class="fab fa-github"></i>
          </a>
        </li>
        <!--
        <li class="list-inline-item social-google-plus">
          <a href="/contact/">
            <i class="fa fa-envelope"></i>
          </a>
        </li>
        -->
      </ul>
    </div>
  </section>

  <footer>
    <div class="container">
      <p>&copy; Yim ML 2020. All Rights Reserved.</p>
      <ul class="list-inline">
        <!-- <li class="list-inline-item">-->
          <!--<a href="#">Privacy</a>-->
        <!--</li> -->

      </ul>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="/static/projects/jquery.min.js"></script>
  <script src="/static/projects/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="/static/projects/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="/static/projects/new-age.min.js"></script>

</body>

</html>

