

<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Yim Machine Learning</title>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" type="text/css" href="/static/projects/bootstrap.min.css">

  <!-- Custom fonts for this template -->
  <link href="/static/projects/all_new.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/static/projects/simple-line-icons_new.css">
  <link href="/static/projects/fonts/googleapis_Lato.txt" rel="stylesheet">
  <link href="/static/projects/fonts/googleapis_Catamaran.txt" rel="stylesheet">
  <link href="/static/projects/fonts/googleapis_Multi.txt" rel="stylesheet">

  <!-- Plugin CSS -->
  <link rel="stylesheet" href="/static/projects/device-mockups.min.css">

  <!-- Custom styles for this template -->
  <link href="/static/projects/new-age.min.css" rel="stylesheet">
  <style>

    header.masthead{position:relative;width:100%;padding-top:150px;padding-bottom:100px;color:#fff;background:url(/static/projects/images/bg-pattern.png),#7b4397;background:url(/static/projects/images/bg-pattern.png),linear-gradient(to left,#7b4397,#dc2430)}

  .center_img{
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
    margin-bottom:50px;
  }  
    </style>
</head>

<body id="page-top">

  <!-- Navigation -->
  
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand js-scroll-trigger" href="/">Yim Machine Learning</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/projects#projects">Projects</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="/projects#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
        </li>
      </ul>
    </div>
  </div>
</nav>



<style>
header.masthead{
  min-height:10vh;max-height:50vh;padding-top:15vh;padding-bottom:0;
  }


.features a{
  color:#843d89;
}
</style>

<header class="masthead" >
    <div class="container h-100">
      <div class="row h-100">
        <div class="col-lg-12 my-auto">
          <div class="header-content mx-auto">
          
            <h1 class="mb-5 text-center">Determining Congestion in Busy Intersections</h1>
          
          </div>
        </div>

      </div>
    </div>
  </header>

  <section class="features" id="about">
    <div class="container">
      <!-- colors purple#7c4191#843d89 red #dd2039  -->
      <div class="row">
        <div class="offset-lg-1 col-lg-10 ">
          <div class="device ">

          <!-- START BLOCK HERE-->
          
            <style>
           .video-container { position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden; }
           .video-container iframe, .video-container object, .video-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
         </style>
          <img src="/static/projects/images/traffic_seg_small.gif" class="img-fluid center_img" alt="" style='margin-bottom:30px'>
          <p>We estimate congestion in crowded intersections in real time. For public planning, municipalities want to understand movement of cars and people. In this post, we discuss the why the view pictured above is technically challenging and how we use background subtraction and image segmentation. Our resulting model is both robust and fast.</p>
          <h3>
            So many people!
          </h3>
          <p>
          We always like to be efficient with our resources. If we leverage existing pre-trained models to complete a task, we can make use of all the data and engineering tweaks build into them. There exists many high quality object detection and image classification models, but with this project, those models fail hard. Why?
<ul>
<li><u>Too many people</u> - common object detection models allow up to 100 objects</li>
<li><u>People too small relative to the frame</u> - popular SSD models assume objects are a certain size</li>
<li><u>Wide-view curvature</u> - models trained on straight-edged images have trouble with distortion</li>
</ul>
<p>
We have to train our own model. Fun! In addition, at the start of the project, we were given 3 days to create a prototype, and a healthy bit of pressure can force resourcefulness. Here was our focus in developing the model.
<ul>
<li><u>Efficient with data</u> - we could only manually label limited image sets in the given time</li>
<li><u>Robust to camera views</u> - the camera would be newly installed and may be slightly different than training data</li>

</ul>
          
          </p>
          <h3>Backgrounds are Complicated; Let's Remove Them</h3>
<p>
Image segmentation is ideal for this task. As opposed to object detection, which attempts to provide locations and classes of individual objects on screen, image segmentation just describes assigns each location to a certain class. Image segmentation is useful since we want to know about groups of people rather than individuals. Our model would classify each group of pixels as either background, person, or vehicle. 
</p>

         
          <p>We kill two birds with one stone, dealing with both location-specific data and labeling speed, with background subtraction. Our advantage in this problem is the fact that the camera is stationary. We can collect the background over multiple frames. When we subtract the background off the image, we are left with white blobs of people are cars. These white blobs will look the same in different locations, so training at location A can easily translate to inference at location B. Another way to look at it is that instead of having 3 classes - background, person, and car, we now have 2 classes - person and car, so we need less labeled data for the model to learn from. However, here's the real kicker: we can now label images much faster! </p>

          <div class='row'>
          <div class="col-lg-6"><img src="/static/projects/images/traffic_bg_0.png" class="img-fluid center_img" alt="" style='margin-bottom:30px'></div>
          
          <div class="col-lg-6"><img src="/static/projects/images/traffic_bgl_0.png" class="img-fluid center_img" alt="" style='margin-bottom:30px'></div>
          </div>
          <p>We obtain the background by taking an average (or mode) of previous frames. There may still be some random incorrect pixels (for example, if a person stood somewhere quite long), but the backgroundless image is still reasonbly good (and perhaps the person who idles forever should be considered part of the background).</p>
          <p>To get the black and white blobs representing "motion" on screen, we subtract the background from a new frame, and apply some threshold, such that each pixel now becomes either a zero or one.</p>
          <div class='row'>
            <div class="col-lg-4"><img src="/static/projects/images/shibuya01_16999a.png" class="img-fluid center_img" alt="" style='margin-bottom:30px'></div>
            
            <div class="col-lg-4"><img src="/static/projects/images/shibuya01_16999b.png" class="img-fluid center_img" alt="" style='margin-bottom:30px'></div>
            <div class="col-lg-4"><img src="/static/projects/images/shibuya01_16999c.png" class="img-fluid center_img" alt="" style='margin-bottom:30px'></div>
            </div>
          <p>Lastly, we need to differentiate between cars and people. We use a photo editor to delete the cars off (by hand), and we generate a car image (programatically) by comparing what was deleted off. 
            We then use these three images to train an image segmentation neural network. Pictured above, the first image is the input to our model, and two other images are the target outputs for image segmentation.
          </p>
          <p>Note this: the only by-hand labeling was the car deletion. For comparison, labeling an original RBG image by hand took us about one hour per image; deleting cars by hand takes us about one minute per image.
            <u>We reduced labeling time from <i>one hour</i> to <i>one minute!</i></u>
          </p>
          <h3>High-Level + Low-Level Information</h3>
          <p>For image segmentation, we need a framework that can capture both high-level and low-level information and merge the information into the prediction. Hence, frameworks like the UNet are often used. In our case, we used the neural network from <a href="https://arxiv.org/pdf/1906.00446.pdf">the VQ-VAE-2 paper</a> (without the PixelSnail). 
            It would be worth testing different frameworks for further study.</p>
          <img src="/static/projects/images/ss_vqvae.png" class="img-fluid center_img" alt="" style='margin-bottom:30px'>
          <h3>The Result: a Real-Time Heatmap of People</h3>
 
       <p>
       In just 3 days, we develop a working prototype that can extract an approximate density of people and cars on screen at the camera framerate (on a desktop GPU). We exploit a unique aspect of this problem - that the camera is stationary - to obtain a background image. We use the background image to expedite labeling and simplify the model classification task.
       </p>
       <p>
       The model is not perfect. There are two main errors. First, the neural network often misclassifies when people bunch up toward the bottom of the screen (creating a big white blob in backgroundless world). This big white blob can be classified as a vehicle. Though a mistake, it makes sense that our model thinks larger objects are likely to be cars. Second, there are significant lighting changes, this causes the background subtraction step to interpret the changes as objects. We aim to solve this by substituting the background subtraction step with a neural network step that can account for lighting changes; stay tuned for that study!</p>

                <p>
            The video below shows a heatmap of just the people.
          </p>
          <div class="video-container">
          <iframe src="https://www.youtube.com/embed/JkBGTsRYrm0?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
          

                  
          <!-- ENDBLOCK -->
          </div>
        </div>
	      



      </div>
    </div>
  </section>






  <section class="contact bg-primary" id="contact">
    <div class="container">
      <h2>Create together!</h2>
      <ul class="list-inline list-social">
        <li class="list-inline-item social-facebook">
          <a href="https://www.linkedin.com/in/yimeric/">
            <i class="fab fa-linkedin"></i>
          </a>
        </li>
        <li class="list-inline-item social-twitter">
          <a href="https://github.com/eric-yim/">
            <i class="fab fa-github"></i>
          </a>
        </li>
        <li class="list-inline-item social-google-plus">
          <a href="/contact/">
            <i class="fa fa-envelope"></i>
          </a>
        </li>
      </ul>
    </div>
  </section>

  <footer>
    <div class="container">
      <p>&copy; Yim ML 2020. All Rights Reserved.</p>
      <ul class="list-inline">
        <!-- <li class="list-inline-item">-->
          <!--<a href="#">Privacy</a>-->
        <!--</li> -->

      </ul>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="/static/projects/jquery.min.js"></script>
  <script src="/static/projects/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="/static/projects/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="/static/projects/new-age.min.js"></script>

</body>

</html>

